<html>
    <head>
        <meta charset="utf-8">
        <title>nlog(m)</title>
        <link rel="stylesheet" href="./style.css">
    </head>
    <body>
        <!--
        <div>
            <p>
                Placeholder for github-style activity chart
            </p>
        </div>
        <hr/>
        -->
        <div>
            <h1>
                Index
            </h1>
            <ul>
                <li><a href="#eventIsTrusted">event.isTrusted (who is the web for?) 12/26/2018</a></li>
                <li><a href="#localcache">Even more local cache 11/26/2018</a></li>
                <li><a href="#gitrevparse">Understanding git rev-parse 9/7/2018</a></li>
                <li><a href="#toomanylinks">Too Many Links 1/22/2019</a></li>
                <li><a href="#where2git">Where should our code live? 1/22/2019</a></li>
                <!--<li><a href="archiveLinkProperty">Give links an archive alternative in case of death</a></li>-->
                <!--<li><a href="apisAreDead">APIs are dead (with a few notable exceptions)</a></li>-->
                <!--<li><a href="#rtl-ltr">WIP: Working with RTL characters 12/10/2018</a></li>-->
                <!--<li><a href="about.html">About</a></li>-->
            </ul>
        </div>

        <hr/>
          <input type="checkbox" style="display:none" />
          <div id="hidden">
          <h1><a id="eventIsTrusted">event.isTrusted (who is the web for?)</a></h1>
          <details>
            <p><summary style="list-style-type:none">
              Was browsing on Stack Overflow and saw a post asking if you could programmatically click a button on a flipkart page. </summary>I was initially interested because its definitely something I know how to do and don't see anything wrong with making bots for websites. Lots of websites allow bots and without them a lot of valuable data would stay inaccessible. APIs (with a few exceptions) are basically dead. There's no point relying on a website allowing access to its data programmatically and even if it exists as Twitter has shown, building on top of it is building on top of openly hostile quicksand.
            </p>
            <p>
              But (as the poster noted) creating links from the JS console doesn't actually fire the onClick event. Even worse, the site is obfuscated in such a way that finding the actually function which is called is hard. Writing this wants me to go back to that page and find it. Anyways, event.isTrusted is one of the ways sites defend against low-grade scapers (like the SO poster and I guess myself) from scraping their website. And to be clear, I don't support scraping a website that you don't have permission to. They totally have the right to throttle and blocks bots, suspicious activity, etc and I would do the same if I was working on a website with the traffic and price-watchers that FlipKart has.
            </p>
            <p>
              event.isTrusted being an immutable property of an event created at inception of an event piqued my curiousity about browser events and how they were created. My thought is if we can find the parameters used to create the event.isTrusted property and then write a function to manipulate those parameters (maybe there is a flag called fromConsole) then we can create an event with isTrusted. I imagine that approach has already been anticipated; however, if I take the time to trace the rabbit hole as deep as it goes maybe I can find a mistake in there. Anyways.
            </p>
            <p>
              scraping random, hostile websites isn't really the point of this post. Maybe FlipKart isn't even using event.isTrusted. There are a lot of other ways to protect against bots running on your site and SO has a great list of them. The list comes with a precaution of course -- its an arms race but there is no guarantee that you (the website) will win. Browsers, after all, are the users software and there isn't a fool-proof way of determining that your user isn't pretending to be or use something that they are not. (afaik)
            </p>
            <p>
              And what rights do users have on a website. If the analogy is a store, surely there are some rights people in a store have. I don't think thats the best analogy but I guess thats a good place to start.
            </p>
          </details>
        </div>
        </div>

        <div>
        <hr/>
          <input type="checkbox" style="display:none" />
          <div id="hidden">
          <h1><a id="localcache">Hyper Local</a></h1>
          <p>
            <details>
              <p>
                <summary style="list-style-type:none">An even more local cache than localStorage. And why localStorage is such a bad name. And other stuff.</summary>
              </p>

              <p>
                localStorage is a terrible name because it is too developer centric -- a user will never and probably should never touch their localStorage.
              </p>

              <p>
                But thats the best we have in terms of "local" control over data. The more local your application is, the more you are empowering you users.
                All of this is a digression from the specific thing I am writing about
              </p>

              <p>
              Check out the demo here: <a href="./projects/local-files/index.html">link</a>
              What we want to do is create a seamless experience for users who sometimes use local data files and other times use a remote data file. I'm making up a few of the terms of the fly but hopefully they will make sense by the end of it. Currently, when a user navigates to a web page (at least) 2 things are loaded. The web application (why all web pages are now applications is 1. makes no sense and 2. I'll talk more about later) and the users data. Usually there is some authentication before you can get to the data and certain parts of the application but the number of hoops you jump through isn't what we are interested in here. The point is the **experience**. Its load application (waiting) and then load the data the user wants to manipulate/view/create/whatever inside that application. 
              </p>

              <p>
                It doesn't have to be like this. Data should be able to live wherever. On the users computer, in a 3rd party storage solution, on the application server, somewhere else on the local network, whatever. Splitting a users data from their applications will be the defining technical shift of the next decade. We don't have to settle for these bloated, crappy apps deciding what data they need to operate and store. We can and should decide. We need to be in control of our applications. If one so much as makes a network request, I want to be able to inspect and approve every piece of data sent over the wire. Is that too much to ask for? You are using my computing resources and for the most part we have no idea what they are used for. Some are wasted on things I don't want or need, others are wasted maliciously (ads, spyware, mining). 
              </p>

              <p>
                I'll walk though the code here
              </p>

              <pre>
                <code>
        var textFile = null,
        makeTextFile = function (text) {
          var data = new Blob([text], {type: 'text/plain'});

          // If we are replacing a previously generated file we need to
          // manually revoke the object URL to avoid memory leaks.
          if (textFile !== null) {
            window.URL.revokeObjectURL(textFile);
          }

          textFile = window.URL.createObjectURL(data);

          return textFile;
        };

        var create = document.getElementById('create');

        create.addEventListener('click', function () {
            const link = document.createElement('a');
            link.setAttribute('download', 'info.txt');
            link.href = makeTextFile("super yo");
            document.body.appendChild(link);

            // wait for the link to be added to the document
            window.requestAnimationFrame(function () {
              const event = new MouseEvent('click');
                link.dispatchEvent(event);
                document.body.removeChild(link);
              });
          }, false);

        function handleFileSelect(evt) {
            let files = evt.target.files; // FileList object

            // files is a FileList of File objects. List some properties.
            let output = [];
            for (let i = 0, f; f = files[i]; i++) {
              output.push('<li><strong>', escape(f.name), '</strong> (', f.type || 'n/a', ') - ',
                          f.size, ' bytes, last modified: ',
                          f.lastModifiedDate ? f.lastModifiedDate.toLocaleDateString() : 'n/a',
                          '</li>');
            }
            document.getElementById('list').innerHTML = '<ul>' + output.join('') + '</ul>';
          }

          document.getElementById('files').addEventListener('change', handleFileSelect, false);
                </code>
              </pre>
            </details>
          </p>
        </div>
        </div>

        <!-- POST -->
        <div>
            <hr/>
            <p style="text-align: center">
                <input type="checkbox" style="display:none" />
                <div id="hidden">
                    <h1><a id="gitrevparse">Understanding git rev-parse</a></h1>

                    <p>
                      <details>
                          <summary style="list-style-type:none">Working on a pre-commit hook for git and was trying to figure out what</summary>
                      <pre>
                          <code>
                          if git rev-parse --verify HEAD >/dev/null 2>&1
                          then
                              against=HEAD
                          else
                              # Initial commit: diff against an empty tree object
                              against=4b825dc642cb6eb9a060e54bf8d69288fbee4904
                          fi
                          </code>
                      </pre>
                          did. 
                      </p>

                      <p>
                          So <code class="inline-code">git rev-parse</code> is a plumbing command that tries to help users understand the maze of parameters that are passed by normal porcelin commands <code class="inline-code">git log</code> to the plumbing command <code class="inline-code">git rev-list</code> which lists git commit objects. Using <code class="inline-code">git rev-list</code> reminds me why its a plumbing command. Its output and <code class="inline-code">man</code> entry are hard to read at best. <code class="inline-code">git rev-parse --verify HEAD</code> is the most user-friendly way to display what commit hash HEAD is currently pointing to.

                      </p>
                      <p>
                      But why is that useful? Its also a useful way to tell if a git repository is empty (just initialized) or not. And thats why the SHA <code class="inline-code">4b825dc642cb6eb9a060e54bf8d69288fbee4904</code> is so special. It is the representation of an empty git tree object. Its the same across every project; <b>so if you start a new git repo and enable pre-commit hooks against your very first commit, this if-block will make sure your commit is diffed against the correct object.</b> Either the HEAD or if there are no other commits in the project, an empty tree object. 
                      </p>
                  </details>
                </p>
            </div>
        <div>
        <hr/>
          <input type="checkbox" style="display:none" />
          <div id="hidden">
            <h1><a id="toomanylinks">Too Many Links</a></h1>
          <p>
            <details>
              <summary style="list-style-type:none">Information Overload</summary>

                <p>I've reached the point in my life where I have collected more information than I will ever be able to consume. Its very likely that I reached this point long before and it took me a while to catch on that the game was up. I'll still learn things but its hard to justify collecting more things when you know you're past capacity. Its hard to say you are discerning when you aren't discerning enough.</p>
                <p>I suppose the best way to deal with this is to go through everything I have and trim the fat. I don't think I've done a good job being selective or maybe I am holding on to too many things that I don't have a use for. I like the idea of having an archive of everything I have read -- in case I need to reference it or just to build a small personalnet for me to explore. Sometimes there outlets I use for media consumption feel dry and over-used. At the end of the day, I'd like to think that I'm best positioned to make choose what I am interested in next. I believe there is a lot of value in swimming parallel to the current. Its the only way to get out of a rip tide.</p>
                <p>I'd prefer this blog to go unnoticed by most; its the place for my personal opinions and reflection. Maybe someone will stumble across it and find something of value and share it. As long as its shared quietly and slowly, I wouldn't mind. My posts from 2014 look funny 5 years later and I imagine anything I write now will look strange in 5 years as well. </p>
              </details>
            </p>
          </div>
        <hr/>
          <input type="checkbox" style="display:none" />
          <div id="hidden">
            <h1><a id="where2git">Where should our code live</a></h1>
          <p>
            <details>
              <summary style="list-style-type:none">A brief history of code storage and thoughts on the newest entrant, <a href="https://sr.ht">sr.ht</a></summary>

                <p>The idea of "github" or a place to store code on the web isn't really a new one. I guess no one has written a decent history of how people share code so I'll do my best</p>

                <p>...</p>
                <p>Now that we have a good understanding of where we came from lets take a look at the newest contender: <a href="https://sr.ht">sr.ht</a>. It was recently released by Drew DeValut and he describes it as
                  <blockquote>
                    <p>
                    sr.ht is special because itâ€™s extremely modular and flexible, designed with interoperability with the rest of the ecosystem in mind. On top of that, sr.ht is one of the most lightweight websites on the internet, with the average page weighing less than 10 KiB, with no tracking and no JavaScript. Each component - git hosting, continuous integration, etc - is a standalone piece of software that integrates deeply with the rest of sr.ht and with the rest of the ecosystem outside of sr.ht.
                    </p>
                  </blockquote>
                  I was initially drawn to it because
                </p>
              </details>
            </p>
          </div>
        </div>
      </div>
  </body>
</html>
